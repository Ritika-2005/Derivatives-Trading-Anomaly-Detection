{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a362dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# CONFIG: adapt connection string\n",
    "engine = create_engine(\"postgresql+psycopg2://user:pass@host:5432/dbname\")\n",
    "symbol = \"BANKNIFTY\"   # change per-run\n",
    "\n",
    "# 1. Load labeled minute data\n",
    "sql = f\"SELECT * FROM labeled_minute WHERE symbol = '{symbol}' ORDER BY minute_ts\"\n",
    "df = pd.read_sql(sql, engine, parse_dates=['minute_ts'])\n",
    "\n",
    "# 2. Ensure datetime index and add minute-of-day\n",
    "df['minute_of_day'] = df['minute_ts'].dt.hour * 60 + df['minute_ts'].dt.minute\n",
    "df = df.set_index('minute_ts')\n",
    "\n",
    "# 3. Rolling features (within day windows)\n",
    "df['price_ret_1m'] = df['price_avg'].pct_change(1)\n",
    "df['price_ret_5m'] = df['price_avg'].pct_change(5)\n",
    "df['vol_5m'] = df['volume'].rolling(window=5, min_periods=1).sum()\n",
    "df['vwap_1m'] = df['vwap']  # already vwap per minute\n",
    "\n",
    "# 4. Baseline VWAP by minute_of_day over previous 20 trading days\n",
    "# Create a helper: compute baseline mean & std per minute_of_day using trailing 20 business days\n",
    "def compute_day_baselines(df, lookback_days=20):\n",
    "    df = df.reset_index()\n",
    "    df['date'] = df['minute_ts'].dt.date\n",
    "    # pivot minute_of_day x date -> vwap; then apply rolling on columns (dates)\n",
    "    pivot = df.pivot_table(index='minute_of_day', columns='date', values='vwap')\n",
    "    # sort columns (dates) ascending\n",
    "    pivot = pivot.sort_index(axis=1)\n",
    "    # compute rolling mean/std across columns\n",
    "    baseline_mean = pivot.T.rolling(window=lookback_days, min_periods=5).mean().T\n",
    "    baseline_std  = pivot.T.rolling(window=lookback_days, min_periods=5).std().T\n",
    "    # melt back into dataframe\n",
    "    baseline_mean = baseline_mean.stack().reset_index().rename(columns={0:'vwap_base_mean'})\n",
    "    baseline_std  = baseline_std.stack().reset_index().rename(columns={0:'vwap_base_std'})\n",
    "    baselines = baseline_mean.merge(baseline_std, on=['minute_of_day','date'])\n",
    "    baselines['minute_ts'] = pd.to_datetime(baselines['date']) + pd.to_timedelta(baselines['minute_of_day'], unit='m')\n",
    "    return baselines.set_index('minute_ts')[['vwap_base_mean','vwap_base_std']]\n",
    "\n",
    "baselines = compute_day_baselines(df, lookback_days=20)\n",
    "df = df.merge(baselines, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# 5. VWAP deviation z-score and relative volume\n",
    "df['vwap_dev'] = (df['vwap'] - df['vwap_base_mean']) / df['vwap_base_std']\n",
    "df['rel_vol_5m'] = df['vol_5m'] / (df['vol_5m'].rolling(window=20*390, min_periods=1).median())  # crude: compare to 20 days of same-minute volumes (optional refine)\n",
    "\n",
    "# 6. OBV (on-balance volume) slope over short window\n",
    "df['side_sign'] = np.sign(df['price_ret_1m'].fillna(0))\n",
    "df['obv'] = (df['volume'] * df['side_sign']).cumsum()\n",
    "df['obv_slope_5m'] = df['obv'].diff(5)\n",
    "\n",
    "# 7. Delta OI for options (pull from DB)\n",
    "sql_oi = f\"SELECT minute_ts, strike, expiry, oi FROM options_oi_minute WHERE symbol LIKE '%{symbol}%' ORDER BY minute_ts\"\n",
    "oi = pd.read_sql(sql_oi, engine, parse_dates=['minute_ts']).set_index('minute_ts')\n",
    "# compute ΔOI per minute for ATM (example: choose strike nearest to underlying current price)\n",
    "# For MVP, compute total ΔOI across all strikes per minute\n",
    "oi_minute = oi.groupby('minute_ts')['oi'].sum().diff().rename('delta_oi')\n",
    "df = df.merge(oi_minute, left_index=True, right_index=True, how='left')\n",
    "\n",
    "# 8. Block trade share\n",
    "df['block_share'] = df['block_qty'] / df['volume'].replace(0, np.nan)\n",
    "\n",
    "# 9. Final feature dropna & save\n",
    "feat_cols = ['vwap','price_avg','price_ret_1m','price_ret_5m','vol_5m','vwap_dev','rel_vol_5m','obv_slope_5m','delta_oi','block_share','label']\n",
    "df_feat = df[feat_cols].reset_index()\n",
    "df_feat.to_csv(f'/tmp/features_{symbol}.csv', index=False)\n",
    "print(\"Saved features:\", df_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab2b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline_detectors.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "df = pd.read_csv('/tmp/features_BANKNIFTY.csv', parse_dates=['minute_ts'])\n",
    "# 1. Z-score rule on vwap_dev AND rel_vol\n",
    "df['z_alert'] = ((df['vwap_dev'].abs() > 3) & (df['rel_vol_5m'] > 2)).astype(int)\n",
    "\n",
    "# 2. Simple CUSUM on price_avg (one-sided up/down)\n",
    "def cusum(series, threshold=0.002):\n",
    "    pos, neg = 0.0, 0.0\n",
    "    out = []\n",
    "    for i in range(1,len(series)):\n",
    "        diff = series.iloc[i] - series.iloc[i-1]\n",
    "        pos = max(0, pos + diff - threshold)\n",
    "        neg = min(0, neg + diff + threshold)\n",
    "        out.append(1 if (pos>0.01 or abs(neg)>0.01) else 0)\n",
    "    return pd.Series([0] + out)\n",
    "\n",
    "df['cusum_alert'] = cusum(df['price_avg'])\n",
    "\n",
    "# 3. Combine detectors\n",
    "df['rule_alert'] = ((df['z_alert']==1) | (df['cusum_alert']==1)).astype(int)\n",
    "\n",
    "# 4. Evaluate (Precision@k)\n",
    "# For Precision@k, pick top k minutes by detector score or just evaluate simple metrics\n",
    "print(\"Rule alerts total:\", df['rule_alert'].sum())\n",
    "print(\"Label positives:\", df['label'].sum())\n",
    "\n",
    "# Precision (naive)\n",
    "if df['rule_alert'].sum()>0:\n",
    "    print(\"Precision:\", precision_score(df['label'], df['rule_alert']))\n",
    "\n",
    "# False positives per month (approx)\n",
    "df['month'] = pd.to_datetime(df['minute_ts']).dt.to_period('M')\n",
    "fp_per_month = df[(df['rule_alert']==1) & (df['label']==0)].groupby('month').size()\n",
    "print(\"FP/month (sample):\")\n",
    "print(fp_per_month.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ba23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf = IsolationForest(contamination=0.01, random_state=42)\n",
    "X = df[['vwap_dev','rel_vol_5m','obv_slope_5m','delta_oi','block_share']].fillna(0)\n",
    "df['iso_anom'] = clf.fit_predict(X)  # -1 anomaly, 1 normal\n",
    "df['iso_alert'] = (df['iso_anom'] == -1).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c72ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# triage_model.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "df = pd.read_csv('/tmp/features_BANKNIFTY.csv', parse_dates=['minute_ts'])\n",
    "# create groups per expiry-day or date for walk-forward CV\n",
    "df['date'] = pd.to_datetime(df['minute_ts']).dt.date\n",
    "# label is 1 for minutes in SEBI windows\n",
    "\n",
    "feat_cols = ['vwap_dev','rel_vol_5m','obv_slope_5m','delta_oi','block_share','price_ret_1m','price_ret_5m']\n",
    "X = df[feat_cols].fillna(0)\n",
    "y = df['label'].fillna(0)\n",
    "\n",
    "# GroupKFold by date to simulate walk-forward (approx)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "scores = []\n",
    "fold = 0\n",
    "for train_idx, test_idx in gkf.split(X, y, groups=df['date']):\n",
    "    fold += 1\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model = xgb.XGBClassifier(n_estimators=200, max_depth=4, learning_rate=0.1, use_label_encoder=False, eval_metric='logloss')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    prob = model.predict_proba(X_test)[:,1]\n",
    "    pred = (prob > 0.5).astype(int)  # tune threshold\n",
    "    print(f\"Fold {fold} AUC:\", roc_auc_score(y_test, prob))\n",
    "    print(f\"Fold {fold} Precision:\", precision_score(y_test, pred, zero_division=0))\n",
    "    scores.append((roc_auc_score(y_test, prob), precision_score(y_test, pred, zero_division=0)))\n",
    "\n",
    "# Train final model on all data\n",
    "final_model = xgb.XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
    "final_model.fit(X, y)\n",
    "\n",
    "# SHAP explainability for per-alert cards\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "# save important features\n",
    "shap.summary_plot(shap_values, X, show=False)  # or save to file in notebook\n",
    "\n",
    "# Create simple alert cards for top predicted minutes\n",
    "df['prob'] = final_model.predict_proba(X)[:,1]\n",
    "alerts = df.sort_values('prob', ascending=False).head(50)\n",
    "alerts[['minute_ts','symbol','prob','label']+feat_cols].to_csv('/tmp/top_alerts.csv', index=False)\n",
    "print(\"Top alerts saved to /tmp/top_alerts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6e88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(df, k=20):\n",
    "    topk = df.sort_values('prob', ascending=False).head(k)\n",
    "    return topk['label'].sum() / k\n",
    "\n",
    "print(\"Precision@20:\", precision_at_k(df,20))\n",
    "\n",
    "\n",
    "events = read_events_somehow()  # from sebi_events table\n",
    "def lead_time_for_event(event_row, alerts_df, threshold=0.5):\n",
    "    # find earliest alert with prob>threshold in (event_start - prebuffer, event_end)\n",
    "    start = event_row['start_ts']\n",
    "    cand = alerts_df[(alerts_df['minute_ts'] >= start - pd.Timedelta('60m')) & (alerts_df['prob']>threshold)]\n",
    "    if cand.empty: return None\n",
    "    return (cand.iloc[0]['minute_ts'] - start).total_seconds() / 60.0  # minutes\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
